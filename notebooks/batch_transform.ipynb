{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Lab 8: Steps 3â€“6 in one cell (clean & self-contained) ===\n",
    "import os, time, uuid, boto3\n",
    "from pathlib import Path\n",
    "from sagemaker import Session, image_uris\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "# --- Env/clients ---\n",
    "REGION       = os.environ.get(\"AWS_REGION\", \"ap-northeast-2\")\n",
    "BUCKET       = os.environ[\"BUCKET\"]\n",
    "LABP         = os.environ.get(\"LAB_PREFIX\", \"student\")\n",
    "SM_ROLE_ARN  = os.environ[\"SM_ROLE_ARN\"]\n",
    "S3_ARTIFACTS = os.environ[\"S3_ARTIFACTS\"]\n",
    "\n",
    "boto_sess = boto3.Session(region_name=REGION)\n",
    "sm = boto_sess.client(\"sagemaker\")\n",
    "s3 = boto_sess.client(\"s3\")\n",
    "sagemaker_sess = Session(boto_session=boto_sess)\n",
    "\n",
    "# --- Step 2: resolve your entrypoint (relative to this notebook) ---\n",
    "entry_point_path = Path(\"../sagemaker/code/inference.py\").resolve()\n",
    "assert entry_point_path.exists(), f\"Missing: {entry_point_path}\"\n",
    "\n",
    "# --- Step 3: latest Approved package ARN ---\n",
    "MPG = f\"{LABP}-telco-churn\"\n",
    "r = sm.list_model_packages(\n",
    "    ModelPackageGroupName=MPG,\n",
    "    ModelApprovalStatus=\"Approved\",\n",
    "    SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=1\n",
    ")\n",
    "assert r.get(\"ModelPackageSummaryList\"), \"No Approved package; approve one in Lab 7.\"\n",
    "pkg_arn = r[\"ModelPackageSummaryList\"][0][\"ModelPackageArn\"]\n",
    "\n",
    "# --- Step 4: describe package -> image, model_data, eval json ---\n",
    "info = sm.describe_model_package(ModelPackageName=pkg_arn)\n",
    "image_uri      = info[\"InferenceSpecification\"][\"Containers\"][0][\"Image\"]\n",
    "model_data_url = info[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]\n",
    "eval_json_s3   = info[\"ModelMetrics\"][\"ModelQuality\"][\"Statistics\"][\"S3Uri\"]\n",
    "\n",
    "print(\"ENTRY:\", entry_point_path)\n",
    "print(\"IMAGE:\", image_uri)\n",
    "print(\"MODEL:\", model_data_url)\n",
    "print(\"EVAL :\", eval_json_s3)\n",
    "\n",
    "# --- Step 5: choose input/output S3 (find newest processed test.csv if not set) ---\n",
    "def find_test_input():\n",
    "    cands = []\n",
    "    p = s3.get_paginator(\"list_objects_v2\")\n",
    "    for prefix in [\"data/processed/\", \"artifacts/preprocess/\"]:\n",
    "        for page in p.paginate(Bucket=BUCKET, Prefix=prefix):\n",
    "            for o in page.get(\"Contents\", []):\n",
    "                if o[\"Key\"].endswith(\"test.csv\"):\n",
    "                    cands.append((o[\"LastModified\"], o[\"Key\"]))\n",
    "    assert cands, \"No processed test.csv found.\"\n",
    "    _, key = max(cands, key=lambda x: x[0])\n",
    "    return f\"s3://{BUCKET}/{key}\"\n",
    "\n",
    "input_s3 = globals().get(\"input_s3\") or find_test_input()\n",
    "from datetime import datetime\n",
    "output_s3 = globals().get(\"output_s3\") or f\"{S3_ARTIFACTS.rstrip('/')}/batch/{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}/\"\n",
    "print(\"INPUT:\", input_s3)\n",
    "print(\"OUT  :\", output_s3)\n",
    "\n",
    "# --- Step 6: build Model with your code, then run Batch Transform ---\n",
    "feature_list_s3 = f\"{S3_ARTIFACTS.rstrip('/')}/preprocess/columns.json\"\n",
    "\n",
    "sk_model = SKLearnModel(\n",
    "    model_data=model_data_url,\n",
    "    image_uri=image_uri,\n",
    "    role=SM_ROLE_ARN,\n",
    "    entry_point=entry_point_path.name,\n",
    "    source_dir=str(entry_point_path.parent),\n",
    "    sagemaker_session=sagemaker_sess,\n",
    "    code_location=f\"{S3_ARTIFACTS.rstrip('/')}/code\",   # ðŸ‘ˆ force upload here\n",
    "    env={\n",
    "        \"EVAL_JSON_S3\": eval_json_s3,\n",
    "        \"FEATURE_LIST_JSON\": feature_list_s3\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "transformer = sk_model.transformer(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    strategy=\"MultiRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=output_s3,\n",
    "    max_payload=6,\n",
    "    max_concurrent_transforms=2,\n",
    ")\n",
    "\n",
    "job_name = f\"{LABP}-bt-{uuid.uuid4().hex[:8]}\"\n",
    "transformer.transform(data=input_s3, content_type=\"text/csv\", split_type=\"Line\", job_name=job_name)\n",
    "print(\"Started:\", job_name)\n",
    "\n",
    "# # prove your code is attached\n",
    "# m = sm.describe_model(ModelName=transformer.latest_transform_job.describe()[\"ModelName\"])\n",
    "# env = m[\"PrimaryContainer\"].get(\"Environment\", {})\n",
    "# print(\"PROGRAM:\", env.get(\"SAGEMAKER_PROGRAM\"))\n",
    "# print(\"SUBMIT :\", env.get(\"SAGEMAKER_SUBMIT_DIRECTORY\"))\n",
    "\n",
    "# --- prove your code is attached (robust) ---\n",
    "d = sm.describe_transform_job(TransformJobName=job_name)\n",
    "model_name = d.get(\"ModelName\")\n",
    "\n",
    "env = {}\n",
    "if model_name:\n",
    "    m = sm.describe_model(ModelName=model_name)\n",
    "    env = m.get(\"PrimaryContainer\", {}).get(\"Environment\", {})\n",
    "\n",
    "print(\"PROGRAM:\", env.get(\"SAGEMAKER_PROGRAM\"))\n",
    "print(\"SUBMIT :\", env.get(\"SAGEMAKER_SUBMIT_DIRECTORY\"))\n",
    "\n",
    "\n",
    "# poll status\n",
    "while True:\n",
    "    d = sm.describe_transform_job(TransformJobName=job_name)\n",
    "    status = d[\"TransformJobStatus\"]\n",
    "    print(\"Status:\", status)\n",
    "    if status in (\"Completed\", \"Failed\", \"Stopped\"):\n",
    "        if status != \"Completed\":\n",
    "            print(\"FailureReason:\", d.get(\"FailureReason\"))\n",
    "        break\n",
    "    time.sleep(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: parse an s3://bucket/prefix/ into (bucket, prefix)\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def parse_s3_uri(s3_uri: str):\n",
    "    u = urlparse(s3_uri)\n",
    "    assert u.scheme == \"s3\" and u.netloc, f\"Bad S3 URI: {s3_uri}\"\n",
    "    # Ensure trailing slash prefix behavior for list_objects\n",
    "    prefix = u.path.lstrip(\"/\")\n",
    "    return u.netloc, prefix\n",
    "\n",
    "# We set this earlier in Step 6\n",
    "print(\"Output (we set this in Step 6):\", output_s3)\n",
    "\n",
    "# --- Find .out objects produced by Batch Transform\n",
    "out_bucket, out_prefix = parse_s3_uri(output_s3)\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "out_keys = []\n",
    "for page in paginator.paginate(Bucket=out_bucket, Prefix=out_prefix):\n",
    "    for obj in page.get(\"Contents\", []):\n",
    "        if obj[\"Key\"].endswith(\".out\"):\n",
    "            out_keys.append(obj[\"Key\"])\n",
    "\n",
    "assert out_keys, f\"No .out files found under {output_s3}\"\n",
    "print(f\"Found {len(out_keys)} output file(s). Showing the first one:\")\n",
    "print(\"  s3://%s/%s\" % (out_bucket, out_keys[0]))\n",
    "\n",
    "# --- Read & preview the first ~10 lines\n",
    "resp = s3.get_object(Bucket=out_bucket, Key=out_keys[0])\n",
    "lines = resp[\"Body\"].read().decode(\"utf-8\").splitlines()\n",
    "\n",
    "# Sanity checks on the \"contract\"\n",
    "assert lines, \"Output file is empty.\"\n",
    "header = lines[0].strip()\n",
    "print(\"Header:\", header)\n",
    "if header.lower() != \"proba,pred\":\n",
    "    print(\"WARNING: Unexpected header. Did your inference script change?\")\n",
    "    print(\"First line raw:\", header)\n",
    "\n",
    "print(\"\\nPreview (first 10 lines):\")\n",
    "for row in lines[:10]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
