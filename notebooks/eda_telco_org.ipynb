{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "### Telco Churning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple\n",
    "import io\n",
    "\n",
    "\n",
    "# Display & plotting defaults\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.b — Load project environment variables (auto-source if missing)\n",
    "\n",
    "REQUIRED = [\"AWS_REGION\", \"ACCOUNT_ID\", \"LAB_PREFIX\", \"BUCKET\", \"S3_DATA\", \"S3_ARTIFACTS\"]\n",
    "\n",
    "def _source_env_file(env_file=\"~/mlops-env.sh\", needed=REQUIRED) -> dict:\n",
    "    \"\"\"\n",
    "    If the kernel launched without inherited env vars (common after timeouts),\n",
    "    safely source your Bash env file in a *subshell* and import only the vars we need.\n",
    "    \"\"\"\n",
    "    path = Path(env_file).expanduser()\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "\n",
    "    # Run a login bash, source the file, then print env; parse here\n",
    "    cmd = f\"bash -lc 'set -a; source {shlex.quote(str(path))} >/dev/null 2>&1; env'\"\n",
    "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
    "\n",
    "    seen = {}\n",
    "    for line in out.splitlines():\n",
    "        if \"=\" not in line:\n",
    "            continue\n",
    "        k, v = line.split(\"=\", 1)\n",
    "        if k in needed:\n",
    "            seen[k] = v\n",
    "    return seen\n",
    "\n",
    "# 1) Check what's already in this kernel's environment\n",
    "missing = [k for k in REQUIRED if not os.environ.get(k)]\n",
    "\n",
    "# 2) If anything is missing, try to import from ~/mlops-env.sh *without* requiring a kernel restart\n",
    "if missing:\n",
    "    imported = _source_env_file()\n",
    "    if imported:\n",
    "        os.environ.update(imported)\n",
    "        # Recompute what's still missing after import\n",
    "        missing = [k for k in REQUIRED if not os.environ.get(k)]\n",
    "\n",
    "# 3) If still missing, tell the student exactly how to fix it\n",
    "if missing:\n",
    "    raise OSError(\n",
    "        \"Missing environment variables: \"\n",
    "        + \", \".join(missing)\n",
    "        + \"\\n\\nFix from VS Code Terminal:\\n\"\n",
    "        \"  source ~/mlops-env.sh\\n\"\n",
    "        \"Then in the notebook: Kernel → Restart and re-run this cell.\"\n",
    "    )\n",
    "\n",
    "# 4) Bind stable globals for the rest of the notebook (do not modify later)\n",
    "REGION       = os.environ[\"AWS_REGION\"]\n",
    "ACCOUNT_ID   = os.environ[\"ACCOUNT_ID\"]\n",
    "LAB_PREFIX   = os.environ[\"LAB_PREFIX\"]\n",
    "BUCKET       = os.environ[\"BUCKET\"]\n",
    "S3_DATA      = os.environ[\"S3_DATA\"]\n",
    "S3_ARTIFACTS = os.environ[\"S3_ARTIFACTS\"]\n",
    "\n",
    "print(json.dumps({\n",
    "    \"REGION\": REGION,\n",
    "    \"ACCOUNT_ID\": ACCOUNT_ID,\n",
    "    \"LAB_PREFIX\": LAB_PREFIX,\n",
    "    \"BUCKET\": BUCKET,\n",
    "    \"S3_DATA\": S3_DATA,\n",
    "    \"S3_ARTIFACTS\": S3_ARTIFACTS\n",
    "}, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One session & client for the whole notebook (faster; cleaner; testable)\n",
    "session = boto3.Session(region_name=REGION)\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Sanity check: who am I? (no secrets printed)\n",
    "sts = session.client(\"sts\")\n",
    "who = sts.get_caller_identity()\n",
    "print(\"Caller identity:\", json.dumps(who, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the Telco CSVs live by convention in this course:\n",
    "#   s3://<bucket>/data/raw/telco/\n",
    "# You can override via TELCO_PREFIX in your shell if you used a different folder.\n",
    "TELCO_PREFIX = os.environ.get(\"TELCO_PREFIX\") or f\"{S3_DATA.rstrip('/')}/raw/telco/\"\n",
    "\n",
    "print(\"TELCO_PREFIX:\", TELCO_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_s3_uri(s3_uri: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Turn 's3://bucket/some/prefix/' into ('bucket', 'some/prefix/').\n",
    "    Accepts URIs with or without trailing slash.\n",
    "    \"\"\"\n",
    "    assert s3_uri.startswith(\"s3://\"), f\"Not an s3 URI: {s3_uri}\"\n",
    "    no_scheme = s3_uri[len(\"s3://\") :]\n",
    "    parts = no_scheme.split(\"/\", 1)\n",
    "    bucket = parts[0]\n",
    "    prefix = \"\" if len(parts) == 1 else parts[1]\n",
    "    return bucket, prefix\n",
    "\n",
    "\n",
    "def list_csv_objects(s3_client, bucket: str, prefix: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    List .csv keys under a prefix (handles pagination).\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3_client.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.lower().endswith(\".csv\"):\n",
    "                keys.append(key)\n",
    "        if resp.get(\"IsTruncated\"):\n",
    "            token = resp[\"NextContinuationToken\"]\n",
    "        else:\n",
    "            break\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket, telco_prefix_key = split_s3_uri(TELCO_PREFIX)\n",
    "csv_keys = list_csv_objects(s3, bucket=bucket, prefix=telco_prefix_key)\n",
    "\n",
    "print(f\"Found {len(csv_keys)} CSV file(s) under {TELCO_PREFIX}\")\n",
    "for k in csv_keys[:10]:\n",
    "    print(\" -\", k)\n",
    "\n",
    "if not csv_keys:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No CSVs under {TELCO_PREFIX}\\n\"\n",
    "        \"Upload your Telco CSV to this folder and re-run this cell.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def read_csv_from_s3(s3_client, bucket: str, key: str, **pd_kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Stream a single CSV from S3 into a pandas DataFrame.\n",
    "    Tries utf-8 first, then falls back to latin-1 for quirky files.\n",
    "    \"\"\"\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    body = obj[\"Body\"].read()  # bytes\n",
    "    try:\n",
    "        return pd.read_csv(io.BytesIO(body), encoding=\"utf-8\", **pd_kwargs)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(io.BytesIO(body), encoding=\"latin-1\", **pd_kwargs)\n",
    "\n",
    "\n",
    "TARGET = \"Churn\"  # keep consistent across labs\n",
    "\n",
    "if len(csv_keys) == 1:\n",
    "    print(\"Loading single file…\")\n",
    "    df = read_csv_from_s3(s3, bucket, csv_keys[0], header=0)\n",
    "else:\n",
    "    print(\"Multiple files detected; loading and concatenating…\")\n",
    "    frames = []\n",
    "    for i, key in enumerate(csv_keys, start=1):\n",
    "        print(f\"  [{i}/{len(csv_keys)}] {key}\")\n",
    "        part = read_csv_from_s3(s3, bucket, key, header=0)\n",
    "        frames.append(part)\n",
    "    df = pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Loaded shape: {df.shape}\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nDtypes:\\n\", df.dtypes)\n",
    "\n",
    "if TARGET in df.columns:\n",
    "    print(\"\\nTarget distribution (raw):\")\n",
    "    print(df[TARGET].value_counts(dropna=False))\n",
    "else:\n",
    "    raise KeyError(\n",
    "        f\"Expected target column '{TARGET}' not found. \" \"Check your file(s) and header row.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a reference to the original dataframe from Step 3\n",
    "df_raw = df.copy()\n",
    "\n",
    "# Quick overview\n",
    "print(\"Rows, Cols:\", df_raw.shape)\n",
    "display(df_raw.sample(min(5, len(df_raw)), random_state=42))\n",
    "display(df_raw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on a clean copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "# 1) Coerce 'TotalCharges' to numeric (bad parses become NaN)\n",
    "if \"TotalCharges\" in df.columns:\n",
    "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
    "\n",
    "# 2) Drop identifier columns if present\n",
    "for col in [\"customerID\", \"CustomerID\", \"customer_id\"]:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "# 3) (Optional) Trim surrounding whitespace in string columns\n",
    "for c in df.select_dtypes(include=\"object\").columns:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# 4) Make sure the target exists and is string/categorical\n",
    "TARGET = \"Churn\"\n",
    "if TARGET not in df.columns:\n",
    "    raise KeyError(f\"Expected target column '{TARGET}' not found. Check your header and file(s).\")\n",
    "\n",
    "# Cast target to category (helps some downstream tools)\n",
    "df[TARGET] = df[TARGET].astype(\"category\")\n",
    "\n",
    "# Show the result\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nulls per column (we'll impute later in the processing pipeline)\n",
    "null_summary = df.isna().sum().sort_values(ascending=False)\n",
    "display(null_summary[null_summary > 0].head(15))\n",
    "\n",
    "# Target distribution (class imbalance is typical: more 'No' than 'Yes')\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[TARGET].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows, Cols:\", df_raw.shape)\n",
    "display(df_raw.sample(5, random_state=42))\n",
    "display(df_raw.dtypes)\n",
    "\n",
    "df = df_raw.copy()\n",
    "if \"TotalCharges\" in df.columns:\n",
    "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
    "\n",
    "for col in [\"customerID\", \"CustomerID\", \"customer_id\"]:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"Churn\" in df.columns, \"Expected 'Churn' column\"\n",
    "vc = df[\"Churn\"].value_counts(dropna=False)\n",
    "display(vc.to_frame(\"count\").assign(share=lambda t: (t[\"count\"] / t[\"count\"].sum()).round(3)))\n",
    "\n",
    "sns.barplot(x=vc.index, y=vc.values)\n",
    "plt.title(\"Target distribution: Churn\")\n",
    "plt.xlabel(\"Churn\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "null_counts = df.isna().sum().sort_values(ascending=False)\n",
    "miss = (\n",
    "    null_counts[null_counts > 0]\n",
    "    .to_frame(name=\"null_count\")\n",
    "    .assign(null_rate=lambda t: (t[\"null_count\"] / len(df)).round(4))\n",
    ")\n",
    "\n",
    "if miss.empty:\n",
    "    print(\"No missing values detected.\")\n",
    "else:\n",
    "    display(miss)\n",
    "\n",
    "    # Bar chart (clearer than a full heatmap when few cols have NaNs)\n",
    "    ax = miss.sort_values(\"null_count\").plot(\n",
    "        kind=\"barh\", y=\"null_count\", legend=False, figsize=(8, 4)\n",
    "    )\n",
    "    plt.title(\"Columns with missing values\")\n",
    "    plt.xlabel(\"Rows with NaN\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TotalCharges\" in df.columns:\n",
    "    n_nan = int(df[\"TotalCharges\"].isna().sum())\n",
    "    rate = n_nan / len(df)\n",
    "    print(f\"TotalCharges NaN rows: {n_nan} ({rate:.2%})\")\n",
    "\n",
    "    if n_nan:\n",
    "        cols_to_show = [c for c in [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"] if c in df.columns]\n",
    "        display(df.loc[df[\"TotalCharges\"].isna(), cols_to_show].head(10))\n",
    "\n",
    "        if \"tenure\" in df.columns:\n",
    "            vc = df.loc[df[\"TotalCharges\"].isna(), \"tenure\"].value_counts().sort_index()\n",
    "            print(\"tenure values among TotalCharges==NaN:\")\n",
    "            display(vc.to_frame(\"rows\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = df.isna().sum().sort_values(ascending=False)\n",
    "display(\n",
    "    nulls[nulls > 0]\n",
    "    .to_frame(\"null_count\")\n",
    "    .assign(null_rate=lambda t: (t[\"null_count\"] / len(df)).round(3))\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(df.isna().iloc[:1000], cbar=False)\n",
    "plt.title(\"Missingness map (first 1000 rows)\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows (subset)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c != \"Churn\"]\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "df[num_cols].hist(figsize=(12, 8), bins=30)\n",
    "plt.suptitle(\"Numeric distributions\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def iqr_outliers(s: pd.Series) -> int:\n",
    "    q1, q3 = s.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    return int(((s < lo) | (s > hi)).sum())\n",
    "\n",
    "\n",
    "outlier_summary = pd.Series({c: iqr_outliers(df[c].dropna()) for c in num_cols}).sort_values(\n",
    "    ascending=False\n",
    ")\n",
    "display(outlier_summary.to_frame(\"iqr_outlier_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "cat_cols = [c for c in cat_cols if c != \"Churn\"]\n",
    "\n",
    "\n",
    "def churn_rate_by(cat_col, top=12):\n",
    "    t = (\n",
    "        df.groupby(cat_col)[\"Churn\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .rename(\"share\")\n",
    "        .mul(100)\n",
    "        .reset_index()\n",
    "    )\n",
    "    return t[t[\"Churn\"] == \"Yes\"].sort_values(\"share\", ascending=False).head(top)\n",
    "\n",
    "\n",
    "for col in [c for c in [\"Contract\", \"PaymentMethod\", \"InternetService\"] if c in cat_cols]:\n",
    "    display(churn_rate_by(col))\n",
    "\n",
    "if \"Contract\" in cat_cols:\n",
    "    rates = churn_rate_by(\"Contract\", top=10)\n",
    "    sns.barplot(data=rates, x=\"Contract\", y=\"share\")\n",
    "    plt.title(\"Churn rate by Contract (%)\")\n",
    "    plt.ylabel(\"Churn Yes (%)\")\n",
    "    plt.xlabel(\"Contract\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "df_corr = df.copy()\n",
    "df_corr[\"ChurnBin\"] = (df_corr[\"Churn\"].map({\"Yes\": 1, \"No\": 0})).astype(\"float\")\n",
    "\n",
    "num_corr = (\n",
    "    df_corr[num_cols + [\"ChurnBin\"]]\n",
    "    .corr()[\"ChurnBin\"]\n",
    "    .drop(\"ChurnBin\")\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "display(num_corr.to_frame(\"pearson_corr_with_churn\"))\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "sns.barplot(x=num_corr.values, y=num_corr.index)\n",
    "plt.title(\"Numeric features: correlation with Churn (Yes=1)\")\n",
    "plt.xlabel(\"Pearson r\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def mi_cat(col):\n",
    "    return mutual_info_score(df_corr[col].astype(str), df_corr[\"Churn\"])\n",
    "\n",
    "\n",
    "mi = pd.Series({c: mi_cat(c) for c in cat_cols}).sort_values(ascending=False)\n",
    "display(mi.to_frame(\"mutual_info_with_churn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10 — Build a compact summary JSON and upload it to S3\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# 1) Build the summary from whatever you’ve computed so far\n",
    "summary = {\n",
    "    \"rows\": int(len(df)),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"target\": \"Churn\",\n",
    "    \"target_counts\": df[\"Churn\"].value_counts(dropna=False).to_dict(),\n",
    "    \"null_columns\": {k: int(v) for k, v in df.isna().sum().items() if v > 0},\n",
    "    # Use empty dicts if students skipped the correlation/MI steps\n",
    "    \"top_numeric_corr\": (num_corr.head(5).round(3).to_dict() if \"num_corr\" in globals() else {}),\n",
    "    \"top_categorical_mi\": (mi.head(5).round(3).to_dict() if \"mi\" in globals() else {}),\n",
    "    \"generated_at\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "}\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "# 2) Save beside the notebook (current working dir is usually .../mlops-day2/notebooks)\n",
    "stamp = datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "local_name = f\"eda_notebook_summary_{stamp}.json\"\n",
    "local_path = Path.cwd() / local_name\n",
    "local_path.write_text(json.dumps(summary, indent=2))\n",
    "print(f\"\\nWrote: {local_path}\")\n",
    "\n",
    "# 3) Upload to S3 using the shared client from Step 2\n",
    "#    (credentials come from the instance role / AWS config — no keys in code)\n",
    "key = f\"artifacts/eda/{local_name}\"\n",
    "s3.upload_file(str(local_path), BUCKET, key)\n",
    "s3_uri = f\"s3://{BUCKET}/{key}\"\n",
    "print(f\"Uploaded to: {s3_uri}\")\n",
    "\n",
    "# 4) (Optional) Quick existence check\n",
    "exists = any(\n",
    "    obj.get(\"Key\") == key\n",
    "    for obj in s3.list_objects_v2(Bucket=BUCKET, Prefix=key).get(\"Contents\", [])\n",
    ")\n",
    "print(\"Verify on S3:\", \"OK ✅\" if exists else \"Not found ❌\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
