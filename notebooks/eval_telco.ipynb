{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-healing env loader for notebooks\n",
    "import os, shlex, subprocess, json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_env_from(path=\"~/mlops-env.sh\", required=(\"AWS_REGION\",\"BUCKET\",\"S3_DATA\",\"S3_ARTIFACTS\")):\n",
    "    p = Path(path).expanduser()\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"{p} not found\")\n",
    "    cmd = f\"bash -lc 'set -a; source {shlex.quote(str(p))} >/dev/null 2>&1; env'\"\n",
    "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
    "    got = {}\n",
    "    for line in out.splitlines():\n",
    "        if \"=\" in line:\n",
    "            k,v = line.split(\"=\",1)\n",
    "            if k: got[k]=v\n",
    "    os.environ.update(got)\n",
    "    missing = [k for k in required if not os.environ.get(k)]\n",
    "    if missing:\n",
    "        raise RuntimeError(\"Missing after load_env: \" + \", \".join(missing))\n",
    "    print(json.dumps({k: os.environ[k] for k in required}, indent=2))\n",
    "\n",
    "load_env_from()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- imports\n",
    "import os, io, json, tarfile, tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, precision_recall_curve,\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "import joblib\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "# --- environment (loaded from ~/mlops-env.sh)\n",
    "REGION  = os.environ.get(\"AWS_REGION\", \"ap-northeast-2\")\n",
    "BUCKET  = os.environ[\"BUCKET\"]\n",
    "S3_DATA_PROCESSED = os.environ[\"S3_DATA_PROCESSED\"]   # e.g., s3://.../data/processed\n",
    "S3_ARTIFACTS      = os.environ[\"S3_ARTIFACTS\"]        # e.g., s3://.../artifacts\n",
    "LABP   = os.environ.get(\"LAB_PREFIX\", \"student\")\n",
    "\n",
    "# --- AWS clients\n",
    "boto_sess = boto3.Session(region_name=REGION)\n",
    "s3 = boto_sess.client(\"s3\")\n",
    "sm = boto_sess.client(\"sagemaker\")\n",
    "\n",
    "def parse_s3(uri: str):\n",
    "    \"\"\"Split s3://bucket/key into (bucket, key).\"\"\"\n",
    "    assert uri.startswith(\"s3://\"), f\"Not an s3 uri: {uri}\"\n",
    "    b, k = uri[5:].split(\"/\", 1)\n",
    "    return b, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- list & pick the latest training job with our prefix\n",
    "resp = sm.list_training_jobs(SortBy=\"CreationTime\", SortOrder=\"Descending\", MaxResults=20)\n",
    "jobs = [j for j in resp[\"TrainingJobSummaries\"] if j[\"TrainingJobName\"].startswith(f\"{LABP}-train-\")]\n",
    "if not jobs:\n",
    "    raise SystemExit(\"No training jobs found for this LAB_PREFIX. Run Lab 5 first.\")\n",
    "job_name = jobs[0][\"TrainingJobName\"]\n",
    "print(\"Latest training job:\", job_name)\n",
    "\n",
    "# --- describe to get S3 locations\n",
    "desc = sm.describe_training_job(TrainingJobName=job_name)\n",
    "model_art  = desc[\"ModelArtifacts\"][\"S3ModelArtifacts\"]      # s3://.../model.tar.gz\n",
    "output_pre = desc[\"OutputDataConfig\"][\"S3OutputPath\"]        # s3://.../artifacts/training/\n",
    "output_tar = f\"{output_pre.rstrip('/')}/{job_name}/output/output.tar.gz\"\n",
    "\n",
    "# --- try to fetch metrics.json from output.tar.gz (nice to have; we’ll still recompute)\n",
    "metrics = None\n",
    "try:\n",
    "    b_out, k_out = parse_s3(output_tar)\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        local_tar = Path(td) / \"output.tar.gz\"\n",
    "        s3.download_file(b_out, k_out, str(local_tar))\n",
    "        with tarfile.open(local_tar) as t:\n",
    "            names = [m.name for m in t.getmembers()]\n",
    "            if \"metrics.json\" in names:\n",
    "                metrics = json.load(t.extractfile(\"metrics.json\"))\n",
    "except Exception as e:\n",
    "    print(\"[WARN] Could not read metrics.json from output tar:\", e)\n",
    "\n",
    "print(\"metrics.json (if any):\", json.dumps(metrics, indent=2)[:400], \"...\" if metrics else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- load model.joblib from model.tar.gz\n",
    "b_mod, k_mod = parse_s3(model_art)\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    local_tar = Path(td) / \"model.tar.gz\"\n",
    "    s3.download_file(b_mod, k_mod, str(local_tar))\n",
    "    with tarfile.open(local_tar) as t:\n",
    "        member = next((m for m in t.getmembers() if m.name.endswith(\"model.joblib\")), None)\n",
    "        if not member:\n",
    "            raise FileNotFoundError(\"model.joblib not found in model artifact\")\n",
    "        model_bundle = joblib.load(t.extractfile(member))\n",
    "\n",
    "model = model_bundle[\"model\"]              # scikit-learn LogisticRegression\n",
    "preprocess = model_bundle.get(\"preprocess\")  # may be None; splits are already numeric\n",
    "\n",
    "# --- read processed test split\n",
    "def s3_read_csv(uri: str) -> pd.DataFrame:\n",
    "    b, k = parse_s3(uri)\n",
    "    obj = s3.get_object(Bucket=b, Key=k)\n",
    "    return pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "test_uri = f\"{S3_DATA_PROCESSED.rstrip('/')}/test/test.csv\"\n",
    "df_test = s3_read_csv(test_uri)\n",
    "\n",
    "feature_cols = [c for c in df_test.columns if c != \"label\"]\n",
    "X_test = df_test[feature_cols].astype(\"float64\").to_numpy()\n",
    "y_test = df_test[\"label\"].astype(\"int64\").to_numpy()\n",
    "\n",
    "X_test.shape, y_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- probabilities & default 0.5 decision\n",
    "proba = model.predict_proba(X_test)[:, 1]\n",
    "pred  = (proba >= 0.5).astype(int)\n",
    "\n",
    "roc = roc_auc_score(y_test, proba)\n",
    "pr  = average_precision_score(y_test, proba)\n",
    "cm  = confusion_matrix(y_test, pred)\n",
    "rep = classification_report(y_test, pred, output_dict=True)\n",
    "\n",
    "print(f\"Test ROC AUC: {roc:.3f} | Test PR AUC: {pr:.3f}\")\n",
    "print(\"Confusion matrix @0.5:\\n\", cm)\n",
    "\n",
    "# --- ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title(\"ROC Curve (Test)\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.show()\n",
    "\n",
    "# --- PR curve\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(rec, prec)\n",
    "plt.title(\"Precision–Recall (Test)\"); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluate at the default threshold (0.5) and plot side-by-side\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1) Probabilities and default 0.5 decision\n",
    "proba = model.predict_proba(X_test)[:, 1]\n",
    "pred  = (proba >= 0.5).astype(int)\n",
    "\n",
    "# 2) Scalar scores + confusion matrix\n",
    "roc = roc_auc_score(y_test, proba)                 # ranking quality (threshold-free)\n",
    "pr  = average_precision_score(y_test, proba)       # area under PR curve (a.k.a. AP)\n",
    "cm  = confusion_matrix(y_test, pred)\n",
    "rep = classification_report(y_test, pred, output_dict=True)\n",
    "\n",
    "print(f\"Test ROC AUC: {roc:.3f} | Test PR AUC: {pr:.3f}\")\n",
    "print(\"Confusion matrix @0.5 [TN FP; FN TP]:\\n\", cm)\n",
    "\n",
    "# 3) Curves (side-by-side)\n",
    "fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "prec, rec, _ = precision_recall_curve(y_test, proba)\n",
    "roc_auc = auc(fpr, tpr)                            # equals roc above\n",
    "ap = pr                                            # alias for clarity\n",
    "base_prec = y_test.mean()                          # prevalence baseline for PR\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# Left: ROC\n",
    "axes[0].plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "axes[0].plot([0, 1], [0, 1], \"--\", linewidth=1)\n",
    "axes[0].set_title(\"ROC Curve (Test)\")\n",
    "axes[0].set_xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "axes[0].set_ylabel(\"True Positive Rate (Recall)\")\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Right: Precision–Recall\n",
    "axes[1].plot(rec, prec, label=f\"AP = {ap:.3f}\")\n",
    "axes[1].axhline(base_prec, linestyle=\"--\", linewidth=1, label=f\"Baseline = {base_prec:.3f}\")\n",
    "axes[1].set_title(\"Precision–Recall Curve (Test)\")\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].legend(loc=\"lower left\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- compute P/R/F1 across thresholds\n",
    "def sweep_thresholds(y_true, p):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, p)\n",
    "    thr = np.r_[0.0, thr]  # align lengths\n",
    "    f1  = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "    return pd.DataFrame({\"threshold\": thr, \"precision\": prec, \"recall\": rec, \"f1\": f1})\n",
    "\n",
    "sweep = sweep_thresholds(y_test, proba)\n",
    "\n",
    "# --- pick strategy: best F1 (simple balanced choice)\n",
    "best_idx = int(np.nanargmax(sweep[\"f1\"].values))\n",
    "t_star   = float(sweep.iloc[best_idx][\"threshold\"])\n",
    "\n",
    "# --- visualize the trade-off\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(sweep[\"threshold\"], sweep[\"precision\"], label=\"Precision\")\n",
    "plt.plot(sweep[\"threshold\"], sweep[\"recall\"],    label=\"Recall\")\n",
    "plt.plot(sweep[\"threshold\"], sweep[\"f1\"],        label=\"F1\")\n",
    "plt.axvline(t_star, linestyle=\"--\", label=f\"Best F1 @ {t_star:.2f}\")\n",
    "plt.xlabel(\"Threshold\"); plt.ylabel(\"Score\"); plt.title(\"Threshold Sweep (Test)\"); plt.legend(); plt.show()\n",
    "\n",
    "# --- confusion & report at chosen threshold\n",
    "pred_star = (proba >= t_star).astype(int)\n",
    "cm_star   = confusion_matrix(y_test, pred_star)\n",
    "rep_star  = classification_report(y_test, pred_star, output_dict=True)\n",
    "\n",
    "print(\"Chosen threshold:\", round(t_star, 3))\n",
    "print(\"Confusion matrix @t*:\\n\", cm_star)\n",
    "pd.DataFrame(rep_star).T.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = {\n",
    "    \"job_name\": job_name,\n",
    "    \"generated_at\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"test\": {\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"pr_auc\": float(pr),\n",
    "        \"threshold_default\": 0.5,\n",
    "        \"threshold_star\": t_star,\n",
    "        \"confusion_matrix@0.5\": cm.tolist(),\n",
    "        \"confusion_matrix@t*\": cm_star.tolist(),\n",
    "        \"report@0.5\": rep,          # per-class precision/recall/f1/support\n",
    "        \"report@t*\":  rep_star,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(json.dumps(evaluation, indent=2)[:800], \"...\\n\")\n",
    "\n",
    "eval_prefix = f\"{S3_ARTIFACTS.rstrip('/')}/evaluation/{job_name}/\"\n",
    "b_eval, k_eval = parse_s3(eval_prefix + \"evaluation.json\")\n",
    "s3.put_object(Bucket=b_eval, Key=k_eval, Body=json.dumps(evaluation, indent=2).encode(\"utf-8\"))\n",
    "print(\"Wrote:\", f\"{eval_prefix}evaluation.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the SDK Model Card first; if your region lacks support, use the Markdown fallback below.\n",
    "try:\n",
    "    from sagemaker.model_card import ModelCard, ModelCardStatus\n",
    "    from sagemaker.model_card import (\n",
    "        ModelOverview, BusinessDetails, IntendedUses, TrainingDetails,\n",
    "        EvaluationResult, Metric, ObjectiveFunction, Function\n",
    "    )\n",
    "\n",
    "    model_card_name = f\"{LABP}-telco-churn-{job_name}\"\n",
    "\n",
    "    overview = ModelOverview(\n",
    "        model_id=model_card_name,\n",
    "        model_name=\"Telco Churn - Logistic Regression\",\n",
    "        problem_type=\"BinaryClassification\",\n",
    "        algorithm_type=\"LogisticRegression\",\n",
    "    )\n",
    "    business = BusinessDetails(\n",
    "        business_problem=\"Predict churn risk to prioritize retention outreach.\",\n",
    "        business_goals=\"Improve retention by acting on high-risk customers.\",\n",
    "        stakeholders=[\"Data Science\", \"Marketing\", \"Care Ops\"],\n",
    "    )\n",
    "    uses = IntendedUses(\n",
    "        intended_uses=[\"Batch and real-time scoring of churn probability\"],\n",
    "        factors_affecting_model_efficiency=[\"Pricing changes\", \"Plan changes\"],\n",
    "        risk_rating=\"Low\",\n",
    "    )\n",
    "    train = TrainingDetails(\n",
    "        objective_function=ObjectiveFunction(function=Function(\"binary_classification\"), notes=\"ROC AUC / PR AUC\"),\n",
    "        training_job_details={\"training_job_arn\": desc[\"TrainingJobArn\"]},\n",
    "    )\n",
    "    eval_results = [\n",
    "        EvaluationResult(name=\"Test ROC AUC\", metric=Metric(name=\"roc_auc\", type=\"number\", value=float(roc))),\n",
    "        EvaluationResult(name=\"Test PR AUC\",  metric=Metric(name=\"pr_auc\",  type=\"number\", value=float(pr))),\n",
    "    ]\n",
    "\n",
    "    mc = ModelCard(\n",
    "        name=model_card_name,\n",
    "        status=ModelCardStatus.DRAFT,\n",
    "        model_overview=overview,\n",
    "        business_details=business,\n",
    "        intended_uses=uses,\n",
    "        training_details=train,\n",
    "        evaluation_results=eval_results,\n",
    "    )\n",
    "    mc.create()\n",
    "    print(\"Created/updated Model Card:\", model_card_name)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"[INFO] Falling back to Markdown model card:\", e)\n",
    "    md = f\"\"\"# Model Card — Telco Churn (LogReg)\n",
    "\n",
    "**Training job:** {job_name}\n",
    "**Generated:** {datetime.utcnow().isoformat(timespec=\"seconds\")}Z\n",
    "\n",
    "## Overview\n",
    "Binary classification to predict churn. Algorithm: Logistic Regression.\n",
    "\n",
    "## Data & Processing\n",
    "- Inputs: {S3_DATA_PROCESSED} (train/val/test)\n",
    "- Preprocess: median impute numerics, one-hot encode categoricals, standard-scale numerics\n",
    "\n",
    "## Key Metrics (Test)\n",
    "- ROC AUC: **{roc:.3f}**\n",
    "- PR AUC: **{pr:.3f}**\n",
    "- Threshold (F1*): **{t_star:.2f}**\n",
    "\n",
    "## Confusion Matrix @ t*\n",
    "{cm_star}\n",
    "\n",
    "## Intended Use & Risks\n",
    "Retention targeting; monitor drift after pricing/plan changes; avoid using sensitive attributes.\n",
    "\"\"\"\n",
    "    key = f\"{LABP}/artifacts/model-cards/{job_name}/model_card.md\"\n",
    "    s3.put_object(Bucket=BUCKET, Key=key, Body=md.encode(\"utf-8\"))\n",
    "    print(\"Wrote s3://{}/{}\".format(BUCKET, key))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(\n",
    "    {\n",
    "        \"job\": job_name,\n",
    "        \"test\": {\n",
    "            \"roc_auc\": round(roc, 3),\n",
    "            \"pr_auc\": round(pr, 3),\n",
    "            \"best_threshold_f1\": round(t_star, 3),\n",
    "            \"cm@t*\": cm_star.tolist(),\n",
    "        },\n",
    "    },\n",
    "    indent=2,\n",
    "))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
