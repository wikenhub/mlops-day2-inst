{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "### Telco Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (keep in a single top cell)\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import subprocess\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import yaml\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.b — Load project environment variables (with granular prefixes)\n",
    "\n",
    "REQUIRED_BASE = [\"AWS_REGION\", \"ACCOUNT_ID\", \"LAB_PREFIX\", \"BUCKET\", \"S3_DATA\", \"S3_ARTIFACTS\"]\n",
    "OPTIONAL_FINE = [\"S3_CODE\", \"S3_DATA_RAW\", \"S3_DATA_PROCESSED\", \"S3_ART_PREPROCESS\"]\n",
    "\n",
    "def _source_env_file(env_file=\"~/mlops-env.sh\", wanted=(REQUIRED_BASE + OPTIONAL_FINE)) -> dict:\n",
    "    path = Path(env_file).expanduser()\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    cmd = f\"bash -lc 'set -a; source {shlex.quote(str(path))} >/dev/null 2>&1; env'\"\n",
    "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
    "    seen = {}\n",
    "    for line in out.splitlines():\n",
    "        if \"=\" not in line:\n",
    "            continue\n",
    "        k, v = line.split(\"=\", 1)\n",
    "        if k in wanted:\n",
    "            seen[k] = v\n",
    "    return seen\n",
    "\n",
    "# Import what we can from current kernel env, then from ~/mlops-env.sh if needed\n",
    "missing = [k for k in REQUIRED_BASE if not os.environ.get(k)]\n",
    "if missing:\n",
    "    os.environ.update(_source_env_file())\n",
    "\n",
    "# Error if base vars still missing\n",
    "missing = [k for k in REQUIRED_BASE if not os.environ.get(k)]\n",
    "if missing:\n",
    "    raise OSError(\n",
    "        \"Missing environment variables: \"\n",
    "        + \", \".join(missing)\n",
    "        + \"\\nFix in VS Code Terminal: source ~/mlops-env.sh, then restart the kernel and re-run.\"\n",
    "    )\n",
    "\n",
    "# Bind coarse vars (from earlier labs)\n",
    "REGION       = os.environ[\"AWS_REGION\"]\n",
    "ACCOUNT_ID   = os.environ[\"ACCOUNT_ID\"]\n",
    "LAB_PREFIX   = os.environ[\"LAB_PREFIX\"]\n",
    "BUCKET       = os.environ[\"BUCKET\"]\n",
    "S3_DATA      = os.environ[\"S3_DATA\"]          # s3://.../data\n",
    "S3_ARTIFACTS = os.environ[\"S3_ARTIFACTS\"]     # s3://.../artifacts\n",
    "\n",
    "# Try to read granular vars; if absent, derive them from the coarse ones (backward compatible)\n",
    "S3_CODE            = os.environ.get(\"S3_CODE\") or f\"s3://{BUCKET}/{LAB_PREFIX}/code\"\n",
    "S3_DATA_RAW        = os.environ.get(\"S3_DATA_RAW\") or f\"{S3_DATA.rstrip('/')}/raw\"\n",
    "S3_DATA_PROCESSED  = os.environ.get(\"S3_DATA_PROCESSED\") or f\"{S3_DATA.rstrip('/')}/processed\"\n",
    "S3_ART_PREPROCESS  = os.environ.get(\"S3_ART_PREPROCESS\") or f\"{S3_ARTIFACTS.rstrip('/')}/preprocess\"\n",
    "\n",
    "print(json.dumps({\n",
    "    \"REGION\": REGION, \"ACCOUNT_ID\": ACCOUNT_ID, \"LAB_PREFIX\": LAB_PREFIX, \"BUCKET\": BUCKET,\n",
    "    \"S3_CODE\": S3_CODE, \"S3_DATA_RAW\": S3_DATA_RAW, \"S3_DATA_PROCESSED\": S3_DATA_PROCESSED,\n",
    "    \"S3_ART_PREPROCESS\": S3_ART_PREPROCESS\n",
    "}, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One session & client for the whole notebook (faster; cleaner; testable)\n",
    "session = boto3.Session(region_name=REGION)\n",
    "s3 = session.client(\"s3\")\n",
    "\n",
    "# Sanity check: who am I? (no secrets printed)\n",
    "sts = session.client(\"sts\")\n",
    "who = sts.get_caller_identity()\n",
    "print(\"Caller identity:\", json.dumps(who, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where the Telco CSVs live by convention in this course:\n",
    "#   s3://<bucket>/data/raw/telco/\n",
    "# You can override via TELCO_PREFIX in your shell if you used a different folder.\n",
    "TELCO_PREFIX = os.environ.get(\"TELCO_PREFIX\") or f\"{S3_DATA_RAW.rstrip('/')}/telco/\"\n",
    "print(\"TELCO_PREFIX:\", TELCO_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_s3_uri(s3_uri: str) -> tuple[str, str]:\n",
    "    \"\"\"Turn 's3://bucket/prefix' -> ('bucket','prefix').\"\"\"\n",
    "    if not s3_uri.startswith(\"s3://\"):\n",
    "        raise ValueError(f\"Not an s3 URI: {s3_uri}\")\n",
    "    no_scheme = s3_uri[len(\"s3://\") :]\n",
    "    parts = no_scheme.split(\"/\", 1)\n",
    "    bucket = parts[0]\n",
    "    prefix = \"\" if len(parts) == 1 else parts[1]\n",
    "    return bucket, prefix\n",
    "\n",
    "def list_csv_objects(s3_client, bucket: str, prefix: str) -> list[str]:\n",
    "    \"\"\"List .csv keys under a prefix (handles pagination).\"\"\"\n",
    "    keys, token = [], None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3_client.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            if key.lower().endswith(\".csv\"):\n",
    "                keys.append(key)\n",
    "        if resp.get(\"IsTruncated\"):\n",
    "            token = resp[\"NextContinuationToken\"]\n",
    "        else:\n",
    "            break\n",
    "    return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket, telco_prefix_key = split_s3_uri(TELCO_PREFIX)\n",
    "csv_keys = list_csv_objects(s3, bucket=bucket, prefix=telco_prefix_key)\n",
    "\n",
    "print(f\"Found {len(csv_keys)} CSV file(s) under {TELCO_PREFIX}\")\n",
    "for k in csv_keys[:10]:\n",
    "    print(\" -\", k)\n",
    "\n",
    "if not csv_keys:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No CSVs under {TELCO_PREFIX}\\n\"\n",
    "        \"Upload your Telco CSV to this folder and re-run this cell.\"\n",
    "    )\n",
    "\n",
    "def read_csv_from_s3(s3_client, bucket: str, key: str, **pd_kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Stream a single CSV from S3 into a pandas DataFrame.\n",
    "    Tries utf-8 first, then falls back to latin-1 for quirky files.\n",
    "    \"\"\"\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    body = obj[\"Body\"].read()  # bytes\n",
    "    try:\n",
    "        return pd.read_csv(io.BytesIO(body), encoding=\"utf-8\", **pd_kwargs)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(io.BytesIO(body), encoding=\"latin-1\", **pd_kwargs)\n",
    "\n",
    "TARGET = \"Churn\"  # keep consistent across labs\n",
    "\n",
    "if len(csv_keys) == 1:\n",
    "    print(\"Loading single file…\")\n",
    "    df = read_csv_from_s3(s3, bucket, csv_keys[0], header=0)\n",
    "else:\n",
    "    print(\"Multiple files detected; loading and concatenating…\")\n",
    "    frames = []\n",
    "    for i, key in enumerate(csv_keys, start=1):\n",
    "        print(f\"  [{i}/{len(csv_keys)}] {key}\")\n",
    "        part = read_csv_from_s3(s3, bucket, key, header=0)\n",
    "        frames.append(part)\n",
    "    df = pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Loaded shape: {df.shape}\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nDtypes:\\n\", df.dtypes)\n",
    "\n",
    "if TARGET in df.columns:\n",
    "    print(\"\\nTarget distribution (raw):\")\n",
    "    print(df[TARGET].value_counts(dropna=False))\n",
    "else:\n",
    "    raise KeyError(f\"Expected target column '{TARGET}' not found. \"\n",
    "                   \"Check your file(s) and header row.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a reference to the original dataframe from Step 3\n",
    "df_raw = df.copy()\n",
    "\n",
    "# Quick overview\n",
    "print(\"Rows, Cols:\", df_raw.shape)\n",
    "display(df_raw.sample(min(5, len(df_raw)), random_state=42))\n",
    "display(df_raw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on a clean copy\n",
    "df = df_raw.copy()\n",
    "\n",
    "# 1) Coerce 'TotalCharges' to numeric (bad parses become NaN)\n",
    "if \"TotalCharges\" in df.columns:\n",
    "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
    "\n",
    "# 2) Drop identifier columns if present\n",
    "for col in [\"customerID\", \"CustomerID\", \"customer_id\"]:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "# 3) (Optional) Trim surrounding whitespace in string columns\n",
    "for c in df.select_dtypes(include=\"object\").columns:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# 4) Make sure the target exists and is string/categorical\n",
    "TARGET = \"Churn\"\n",
    "if TARGET not in df.columns:\n",
    "    raise KeyError(f\"Expected target column '{TARGET}' not found. Check your header and file(s).\")\n",
    "\n",
    "# Cast target to category (helps some downstream tools)\n",
    "df[TARGET] = df[TARGET].astype(\"category\")\n",
    "\n",
    "# Show the result\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nulls per column (we'll impute later in the processing pipeline)\n",
    "null_summary = df.isna().sum().sort_values(ascending=False)\n",
    "display(null_summary[null_summary > 0].head(15))\n",
    "\n",
    "# Target distribution (class imbalance is typical: more 'No' than 'Yes')\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[TARGET].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"Churn\" in df.columns, \"Expected 'Churn' column\"\n",
    "vc = df[\"Churn\"].value_counts(dropna=False)\n",
    "display(vc.to_frame(\"count\").assign(share=lambda t: (t[\"count\"]/t[\"count\"].sum()).round(3)))\n",
    "\n",
    "sns.barplot(x=vc.index, y=vc.values)\n",
    "plt.title(\"Target distribution: Churn\"); plt.xlabel(\"Churn\"); plt.ylabel(\"Count\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = df.isna().sum().sort_values(ascending=False)\n",
    "miss = (\n",
    "    null_counts[null_counts > 0]\n",
    "    .to_frame(name=\"null_count\")\n",
    "    .assign(null_rate=lambda t: (t[\"null_count\"] / len(df)).round(4))\n",
    ")\n",
    "\n",
    "if miss.empty:\n",
    "    print(\"No missing values detected.\")\n",
    "else:\n",
    "    display(miss)\n",
    "\n",
    "    # Bar chart (clearer than a full heatmap when few cols have NaNs)\n",
    "    ax = miss.sort_values(\"null_count\").plot(\n",
    "        kind=\"barh\", y=\"null_count\", legend=False, figsize=(8, 4)\n",
    "    )\n",
    "    plt.title(\"Columns with missing values\")\n",
    "    plt.xlabel(\"Rows with NaN\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TotalCharges\" in df.columns:\n",
    "    n_nan = int(df[\"TotalCharges\"].isna().sum())\n",
    "    rate = n_nan / len(df)\n",
    "    print(f\"TotalCharges NaN rows: {n_nan} ({rate:.2%})\")\n",
    "\n",
    "    if n_nan:\n",
    "        cols_to_show = [c for c in [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"] if c in df.columns]\n",
    "        display(df.loc[df[\"TotalCharges\"].isna(), cols_to_show].head(10))\n",
    "\n",
    "        if \"tenure\" in df.columns:\n",
    "            vc = df.loc[df[\"TotalCharges\"].isna(), \"tenure\"].value_counts().sort_index()\n",
    "            print(\"tenure values among TotalCharges==NaN:\")\n",
    "            display(vc.to_frame(\"rows\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not miss.empty and miss[\"null_count\"].sum() > 0:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    # Show at most 1000 rows to keep it readable\n",
    "    subset = df.isna().iloc[: min(1000, len(df))]\n",
    "    sns.heatmap(subset, cbar=False)\n",
    "    plt.title(\"Missingness map (first 1000 rows)\")\n",
    "    plt.xlabel(\"Columns\"); plt.ylabel(\"Rows (subset)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c != \"Churn\"]\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "df[num_cols].hist(figsize=(12, 8), bins=30)\n",
    "plt.suptitle(\"Numeric distributions\", y=1.02); plt.show()\n",
    "\n",
    "def iqr_outliers(s: pd.Series) -> int:\n",
    "    q1, q3 = s.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    return int(((s < lo) | (s > hi)).sum())\n",
    "\n",
    "outlier_summary = pd.Series({c: iqr_outliers(df[c].dropna()) for c in num_cols}).sort_values(ascending=False)\n",
    "display(outlier_summary.to_frame(\"iqr_outlier_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "cat_cols = [c for c in cat_cols if c != \"Churn\"]\n",
    "\n",
    "def churn_rate_by(cat_col, top=12):\n",
    "    t = (df.groupby(cat_col)[\"Churn\"]\n",
    "           .value_counts(normalize=True)\n",
    "           .rename(\"share\")\n",
    "           .mul(100)\n",
    "           .reset_index())\n",
    "    return t[t[\"Churn\"] == \"Yes\"].sort_values(\"share\", ascending=False).head(top)\n",
    "\n",
    "for col in [c for c in [\"Contract\", \"PaymentMethod\", \"InternetService\"] if c in cat_cols]:\n",
    "    display(churn_rate_by(col))\n",
    "\n",
    "if \"Contract\" in cat_cols:\n",
    "    rates = churn_rate_by(\"Contract\", top=10)\n",
    "    sns.barplot(data=rates, x=\"Contract\", y=\"share\")\n",
    "    plt.title(\"Churn rate by Contract (%)\")\n",
    "    plt.ylabel(\"Churn Yes (%)\"); plt.xlabel(\"Contract\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "df_corr = df.copy()\n",
    "df_corr[\"ChurnBin\"] = (df_corr[\"Churn\"].map({\"Yes\": 1, \"No\": 0})).astype(\"float\")\n",
    "\n",
    "num_corr = df_corr[num_cols + [\"ChurnBin\"]].corr()[\"ChurnBin\"].drop(\"ChurnBin\").sort_values(ascending=False)\n",
    "display(num_corr.to_frame(\"pearson_corr_with_churn\"))\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "sns.barplot(x=num_corr.values, y=num_corr.index)\n",
    "plt.title(\"Numeric features: correlation with Churn (Yes=1)\")\n",
    "plt.xlabel(\"Pearson r\"); plt.ylabel(\"Feature\"); plt.show()\n",
    "\n",
    "def mi_cat(col):\n",
    "    return mutual_info_score(df_corr[col].astype(str), df_corr[\"Churn\"])\n",
    "mi = pd.Series({c: mi_cat(c) for c in cat_cols}).sort_values(ascending=False)\n",
    "display(mi.to_frame(\"mutual_info_with_churn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11 — Save concise EDA summary to $S3_ART_PREPROCESS/eda/\n",
    "summary = {\n",
    "    \"rows\": int(len(df)),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"target\": \"Churn\",\n",
    "    \"target_counts\": df[\"Churn\"].value_counts(dropna=False).to_dict(),\n",
    "    \"null_columns\": {k: int(v) for k, v in df.isna().sum().items() if v > 0},\n",
    "    \"top_numeric_corr\": (num_corr.head(5).round(3).to_dict() if \"num_corr\" in globals() else {}),\n",
    "    \"top_categorical_mi\": (mi.head(5).round(3).to_dict() if \"mi\" in globals() else {}),\n",
    "    \"generated_at\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "}\n",
    "print(json.dumps(summary, indent=2))\n",
    "\n",
    "stamp = datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "local_name = f\"eda_notebook_summary_{stamp}.json\"\n",
    "local_path = Path.cwd() / local_name\n",
    "local_path.write_text(json.dumps(summary, indent=2))\n",
    "print(f\"Wrote: {local_path}\")\n",
    "\n",
    "# Upload to $S3_ART_PREPROCESS/eda/\n",
    "eda_bucket, eda_prefix = split_s3_uri(f\"{S3_ART_PREPROCESS.rstrip('/')}/eda/\")\n",
    "eda_key = f\"{eda_prefix.rstrip('/')}/{local_name}\"\n",
    "s3.upload_file(str(local_path), eda_bucket, eda_key)\n",
    "print(\"Uploaded to:\", f\"s3://{eda_bucket}/{eda_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 — Write plan.yaml (decisions derived from EDA) to $S3_ART_PREPROCESS/\n",
    "\n",
    "plan = {\n",
    "    \"target\": \"Churn\",\n",
    "    \"splits\": {\"test_size\": 0.20, \"val_size\": 0.10, \"random_state\": 42, \"stratify\": True},\n",
    "    \"numeric_imputer\": \"median\",              # robust; matches TotalCharges insights\n",
    "    \"categorical_imputer\": \"most_frequent\",   # simple, effective for Telco\n",
    "    \"encoder\": \"onehot_ignore_unknown\",       # avoids crashes at inference\n",
    "    \"scale_numeric\": True,                    # StandardScaler on numeric slice\n",
    "    \"label_mapping\": {\"No\": 0, \"Yes\": 1},\n",
    "    \"class_weight\": \"balanced\",               # handle imbalance\n",
    "    # Optional context for reviewers:\n",
    "    \"notes\": {\n",
    "        \"why_numeric_median\": \"skew & outliers; median is robust\",\n",
    "        \"why_ignore_unknown\": \"stability with new categories at inference\",\n",
    "        \"eda_artifact_hint\": \"see $S3_ART_PREPROCESS/eda/ for report json\"\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save alongside the notebook\n",
    "plan_path = Path.cwd() / \"plan.yaml\"\n",
    "plan_path.write_text(yaml.safe_dump(plan, sort_keys=False))\n",
    "print(\"Local plan.yaml written:\", plan_path)\n",
    "\n",
    "# Upload to $S3_ART_PREPROCESS/plan.yaml\n",
    "pbucket, pprefix = split_s3_uri(S3_ART_PREPROCESS)\n",
    "plan_key = f\"{pprefix.rstrip('/')}/plan.yaml\"\n",
    "s3.upload_file(str(plan_path), pbucket, plan_key)\n",
    "print(\"Uploaded plan to:\", f\"s3://{pbucket}/{plan_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick existence checks\n",
    "resp = s3.list_objects_v2(Bucket=pbucket, Prefix=pprefix.rstrip('/') + \"/\")\n",
    "for obj in resp.get(\"Contents\", []):\n",
    "    if obj[\"Key\"].endswith(\"plan.yaml\") or \"eda_notebook_summary\" in obj[\"Key\"]:\n",
    "        print(\"✓\", obj[\"Key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
