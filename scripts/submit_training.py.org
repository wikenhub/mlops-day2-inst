# scripts/submit_training.py
import os
from datetime import datetime

import boto3

import sagemaker
from sagemaker.inputs import TrainingInput
from sagemaker.sklearn.estimator import SKLearn

# 1) Read environment (from ~/mlops-env.sh in your shell)
REGION = os.environ.get("AWS_REGION", "ap-northeast-2")
BUCKET = os.environ["BUCKET"]
S3_DATA = os.environ["S3_DATA"]  # s3://.../data
S3_ART = os.environ["S3_ARTIFACTS"]  # s3://.../artifacts
ROLE = os.environ["SM_ROLE_ARN"]
LABP = os.environ.get("LAB_PREFIX", "student")

# 2) SageMaker session
boto_sess = boto3.Session(region_name=REGION)
sm_sess = sagemaker.Session(boto_session=boto_sess)

# 3) Define the estimator (uses the scikit-learn container)
est = SKLearn(
    entry_point="sagemaker/code/train.py",
    role=ROLE,
    instance_type="ml.m5.large",
    instance_count=1,
    framework_version="1.2-1",
    py_version="py3",
    sagemaker_session=sm_sess,
    base_job_name=f"{LABP}-train",
    # Add these so artifacts land in YOUR bucket, not the default one
    output_path=f"{S3_ART}/training/",  # where model.tar.gz + output.tar.gz go
    code_location=f"{S3_ART}/code/",  # where SageMaker uploads your source tarball
    hyperparameters={
        "target": "Churn",
        "max-iter": 200,
        "C": 1.0,
        "penalty": "l2",
        "solver": "lbfgs",
        "class-weights": "auto",
        "random-state": 42,
    },
)


# 4) Map S3 prefixes to training channels
inputs = {
    "train": TrainingInput(f"{S3_DATA}/processed/train/"),
    "val": TrainingInput(f"{S3_DATA}/processed/val/"),
    "test": TrainingInput(f"{S3_DATA}/processed/test/"),
    "artifacts": TrainingInput(f"{S3_ART}/preprocess/"),
}

# 5) Launch
job_name = f"{LABP}-train-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"
print("Submitting job:", job_name)
est.fit(inputs=inputs, job_name=job_name, wait=True, logs="All")
print("Training job finished.")
